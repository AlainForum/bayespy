% This file was created with JabRef 2.10b2.
% Encoding: ISO8859_1


@InProceedings{Hensman:2012,
  Title                    = {Fast Variational Inference in the Conjugate Exponential Family},
  Author                   = {Hensman, James and Rattray, Magnus and Lawrence, Neil D.},
  Booktitle                = {Advances in Neural Information Processing Systems 25},
  Year                     = {2012},

  Address                  = {Lake Tahoe, Nevada, USA},
  Editor                   = {Pereira, F. and Burges, C.J.C. and Bottou, L. and Weinberger, K.Q.},
  Pages                    = {2888--2896}
}

@InCollection{Luttinen:2013,
  Title                    = {Fast Variational {Bayesian} Linear State-Space Model},
  Author                   = {Luttinen, Jaakko},
  Booktitle                = {Machine Learning and Knowledge Discovery in Databases},
  Publisher                = {Springer},
  Year                     = {2013},
  Editor                   = {Blockeel, Hendrik and Kersting, Kristian and Nijssen, Siegfried and Železný, Filip},
  Pages                    = {305--320},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {8188},

  Abstract                 = {This paper presents a fast variational Bayesian method for linear state-space models. The standard variational Bayesian expectation-maximization (VB-EM) algorithm is improved by a parameter expansion which optimizes the rotation of the latent space. With this approach, the inference is orders of magnitude faster than the standard method. The speed of the proposed method is demonstrated on an artificial dataset and a large real-world dataset, which shows that the standard VB-EM algorithm is not suitable for large datasets because it converges extremely slowly. In addition, the paper estimates the temporal state variables using a smoothing algorithm based on the block LDL decomposition. This smoothing algorithm reduces the number of required matrix inversions and avoids a model augmentation compared to previous approaches.},
  Doi                      = {10.1007/978-3-642-40988-2_20}
}

@Article{Luttinen:2010,
  Title                    = {Transformations in variational {Bayesian} factor analysis to speed up learning},
  Author                   = {Luttinen, Jaakko and Ilin, Alexander},
  Journal                  = {Neurocomputing},
  Year                     = {2010},
  Pages                    = {1093--1102},
  Volume                   = {73},

  Abstract                 = {We propose simple transformation of the hidden states in variational Bayesian factor analysis models to speed up the learning procedure. The speed-up is achieved by using proper parameterization of the posterior approximation which allows joint optimization of its individual factors, thus the transformation is theoretically justified. We derive the transformation formulae for variational Bayesian factor analysis and show experimentally that it can significantly improve the rate of convergence. The proposed transformation basically performs centering and whitening of the hidden factors taking into account the posterior uncertainties. Similar transformations can be applied to other variational Bayesian factor analysis models as well.},
  Doi                      = {10.1016/j.neucom.2009.11.018}
}

@InCollection{Luttinen:2014,
  Title                    = {Linear State-Space Model with Time-Varying Dynamics},
  Author                   = {Luttinen, Jaakko and Raiko, Tapani and Ilin, Alexander},
  Booktitle                = {Machine Learning and Knowledge Discovery in Databases},
  Publisher                = {Springer},
  Year                     = {2014},
  Editor                   = {Calders, Toon and Esposito, Floriana and H\"ullermeier, Eyke and Meo, Rosa},
  Note                     = {To appear},
  Pages                    = {338--353},
  Series                   = {Lecture Notes in Computer Science},
  Volume                   = {8725},

  Abstract                 = {This paper introduces a linear state-space model with time-varying dynamics. The time dependency is obtained by forming the state dynamics matrix as a time-varying linear combination of a set of matrices. The time dependency of the weights in the linear combination is modelled by another linear Gaussian dynamical model allowing the model to learn how the dynamics of the process changes. Previous approaches have used switching models which have a small set of possible state dynamics matrices and the model selects one of those matrices at each time, thus jumping between them. Our model forms the dynamics as a linear combination and the changes can be smooth and more continuous. The model is motivated by physical processes which are described by linear partial differential equations whose parameters vary in time. An example of such a process could be a temperature field whose evolution is driven by a varying wind direction. The posterior inference is performed using variational Bayesian approximation. The experiments on stochastic advection-diffusion processes and real-world weather processes show that the model with time-varying dynamics can outperform previously introduced approaches.},
  Doi                      = {10.1007/978-3-662-44851-9_22}
}

